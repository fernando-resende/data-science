# -*- coding: utf-8 -*-
"""Covid-19 - MG - Demográfico

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_NwgS3hUGhZ5EkoPEMcYIVhw1uO0Rb9T

# Análise e predição de recuperação e óbitos da Covid-19 em Minas Gerais

## Orientação
Este notebook foi criado no ambiente *Google Colab*, portanto para garantir o correto funcionamento recomenda-se que seja executado neste ambiente.

## Introdução
Trabalho de conclusão de curso de pós graduação em Ciência de dados e Big Data na PUC Minas - Virtual.

## Objetivo
Analisar os dados de óbitos e recuperações nas cidades e regiões de Minas Gerais e gerar modelos de machine learning (ML) para prever se determinado paciente infectado evoluirá para óbito ou recuperação.

## Preparação do ambiente
Importações de bibliotecas
"""

import pandas as pd
import numpy as np
import datetime as dt
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier

np.random.seed(9483) # semente padrão para estado aleatório
sns.set_theme() # tema padrão do seaborn
plt.rc("figure", figsize=(12,8)) # tamanho padrão de plotagem
color_palette = 'Reds_r' # paleta de cores padrão para os gráficos
fig_size = {'w':8,'h':5} # tamanho padrão para uso nos gráficos

"""## Obtenção dos dados

### Secretaria de Estado de Saúde de Minas Gerais - SES

Serão obtidos dados de recuperados, em acompanhamento e óbitos causados pela Covid-19 nos municípios de Minas Gerais.

Os dados são disponibilizados publicamente pela Secretaria de Estado de Saúde de Minas Gerais - SES, em formato *xlsx*, disponível em [https://coronavirus.saude.mg.gov.br/images/microdados/xlsx_sistemas.xlsx](https://coronavirus.saude.mg.gov.br/images/microdados/xlsx_sistemas.xlsx).

O arquivo é uma pasta de trabalho com apenas uma planilha, contendo informações específicas sobre os infectados da Covid-19 nos municípios.

Para este trabalho serão considerados os dados atualizados em 23/04/2021. O arquivo está disponível em ['https://github.com/fernando-resende/data-science/blob/main/covid-19/classification/datasets/xlsx_sistemas.xlsx?raw=true']('https://github.com/fernando-resende/data-science/blob/main/covid-19/classification/datasets/xlsx_sistemas.xlsx?raw=true').

Caso prefira avaliar os dados atuais, passe *True* como valor do parâmetro *download_from_source* na função *get_covid_dataframe*.

**Atenção:** arquivo grande, leva em média mais de 30 segundos para ser carregado.
"""

# Se download_from_source = True, baixará os arquivos da fonte original
def get_covid_dataframe(download_from_source = False):
  if not download_from_source:
    covid_file_path = 'https://github.com/fernando-resende/data-science/blob/main/covid-19/classification/datasets/xlsx_sistemas.xlsx?raw=true'
  else:
    covid_file_path = 'https://coronavirus.saude.mg.gov.br/images/microdados/xlsx_sistemas.xlsx'
  return pd.read_excel(covid_file_path)

covid_demographic_mg = get_covid_dataframe()

"""Exibindo amostras aleatórias dos datasets"""

covid_demographic_mg.sample(5)

"""### Web Scraping

Um dos requisitos do trabalho é utilizar técnica de web scraping para obter dados e enriquecer o(s) dataset(s).

Será utilizada uma fonte de dados com informações sobre a população estimada, salário médio e Índice de Desenvolvimento Humano Municipal (IDHM) nas cidades do estado de Minas Gerais.

Os dados serão obtidos do site do Instituto Brasileiro de Geografia e Estatística - IBGE, disponíveis no seguinte endereço [https://cidades.ibge.gov.br/brasil/sintese/mg?indicadores=29171,30255,29765](https://cidades.ibge.gov.br/brasil/sintese/mg?indicadores=29171,30255,29765).

A fonte de dados em questão é apresentada em forma de tabela, porém os dados são carregados dinamicamente. Dessa forma, o uso da biblioteca *requests* para obter a página web diretamente da fonte não atende, pois não consegue tratar carregamentos dinâmicos, resultando em uma tabela vazia quando utilizada.

Dessa forma, optou-se por gerar um arquivo HTML estático e disponibilizá-lo no GitHub, para então fazer uso das bibliotecas *requests* e *BeautifullSoup*. Arquivo disponível em [https://github.com/fernando-resende/data-science/blob/main/covid-19/classification/datasets/IBGE_Municipios_MG.html](https://github.com/fernando-resende/data-science/blob/main/covid-19/classification/datasets/IBGE_Municipios_MG.html).

Ainda há a opção de obter os dados diretamente da fonte pelo uso da biblioteca *Selenium*, que suporta páginas com conteúdo dinâmico, caso queira passe *True* como valor do parâmetro *download_from_source* na função *web_scraping_mg_cities*.

Caso opte pela obtenção dos dados da fonte original, serão feitas instalações, importações e configurações necessárias para execução no ambiente Google Colab. 

**Atenção:** a inicialização da sessão pelo *web driver* do *Selenium*, geração dinâmica dos dados, coleta e conversão em dataframe leva, em média, mais de 45 segundos. Caso o resultado da execução sejam apenas duas colunas sem conteúdo, pode ser que o site esteja fora do ar, como ocorrido em 05/02/2021, neste caso deve-se executar novamente a célula ou utilizar a fonte alternativa (GitHub).
"""

def web_scraping_mg_cities(download_from_source = False):
  if not download_from_source:
    # Site do IBGE em manutenção em 05/02/2020
    # Contornando com arquivo do Github com a tabela dos municípios
    from bs4 import BeautifulSoup
    import requests

    try:
      response = requests.get('https://raw.githubusercontent.com/fernando-resende/data-science/main/covid-19/classification/datasets/IBGE_Municipios_MG.html')
      municipios_html = BeautifulSoup(response.text, 'html.parser')

      table_elements = municipios_html.select('tbody')
      data_list_IBGE = []

      for element in table_elements:
        row = []
        for cell in element.select('tr > *'):
          row.append(cell.text)
        data_list_IBGE.append(row)

      cities = pd.DataFrame(data_list_IBGE)
      cities.columns = [x.text for x in municipios_html.select('thead > tr > th')]
    except requests.exceptions.RequestException as e:
      print("Ocorreu um erro.\nExecute novamente a célula ou alterere a variável 'download_from_source' para obter os dados da fonte original.")
      print("Detalhes:\n",e)
      return None

  else:
    # https://stackoverflow.com/questions/51046454/how-can-we-use-selenium-webdriver-in-colab-research-google-com
    !pip install selenium
    !apt-get update 
    !apt install chromium-chromedriver

    from selenium import webdriver
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    web_driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)

    web_driver.get("https://cidades.ibge.gov.br/brasil/sintese/mg?indicadores=29171,30255,29765")
    table_elements = web_driver.find_elements_by_css_selector('#municipios > :not(tfoot)')

    # Em algumas situações o web driver não consegue carregar o conteúdo gerado dinamicamente, nesse caso executa-se novamente
    if (len(table_elements)<3):
      table_elements = get_IBGE_table(web_driver)
      if (len(table_elements)<3):
        table_elements = None

    data_list_IBGE = []

    if table_elements != None:
      for element in table_elements:
        row = []
        for cell in element.find_elements_by_css_selector('tr > *'):
          row.append(cell.text)
        data_list_IBGE.append(row)

      cities = pd.DataFrame(data_list_IBGE)
      cities.columns = cities.loc[0]
      cities = cities.drop(0)

  return cities

ibge_cities_mg = web_scraping_mg_cities()
ibge_cities_mg.head()

"""## Tratamento dos dados
Serão verificadas descrições dos dataframes, se existem valores ausentes e outras verificações julgadas relevantes.

Remoções, inserções, ajustes e outras ações nos dataframes poderão ser realizados durante o processo de tratamento.

### Dados da covid-19
Verificando informações do dataset.
"""

print('Descrição do dataset demográfico da covid-19 em MG\n')
print(covid_demographic_mg.info())

"""Observa-se que o dataframe possui valores ausentes em praticamente todas as colunas.

Em especial há a coluna "*DATA_EVOLUÇÃO*" que apresenta maior quantidade de dados ausentes.

Antes de iniciar o tratamento de dados ausentes serão removidas as linhas de pessoas em acompanhamento pois o objetivo é prever recuperação e óbito apenas. Os dados obtidos dessa ação serão inseridos em um novo dataframe.
"""

cases_evolution_mg = pd.DataFrame(covid_demographic_mg.query("EVOLUCAO != 'EM ACOMPANHAMENTO'"))
cases_evolution_mg.head()

"""Após visualização dos primeiros registros do dataframe nota-se que nem todos os casos recuperados possuem o dado da data de evolução. Será que o mesmo ocorre para os óbitos?"""

cases_evolution_mg.loc[(cases_evolution_mg['EVOLUCAO'] == 'OBITO') & (cases_evolution_mg['DATA_EVOLUCAO'].isna())].head()

"""O fenômeno também ocorre para os casos de óbitos e a coluna "*DATA_EVOLUCAO*" será descartada pois, além de conter muitos dados ausentes, o dado em si não é relevante para o trabalho.

Com o intuito de identificar as demais colunas candidatas a serem descartadas, serão avaliados os registros únicos em cada coluna.
"""

cases_evolution_mg.nunique()

"""Após avaliação nota-se que as colunas "*CLASSIFICACAO_CASO*" e "*DATA_ATUALIZACAO*" apresentam valores únicos e são fortes candidatas a serem removidas pois não agregam informações relevantes."""

cases_evolution_mg[['CLASSIFICACAO_CASO','DATA_ATUALIZACAO']].describe()

"""Além das colunas citadas anteriormente, a coluna "*ID*" corresponde ao idenficiador de cada caso e não agrega informações relevantes para este trabalho, portanto será removida.

A coluna "*IDADE*" também será descartada pois já existem agurpamentos na coluna "*FAIXA_ETARIA*", o que melhora o desempenho dos modelos de Machine Learning (ML) a serem criados posteriormente.

Removendo colunas desnecessárias:
"""

cases_evolution_mg.drop(['ID','CLASSIFICACAO_CASO','IDADE','DATA_EVOLUCAO','DATA_ATUALIZACAO'], axis=1, inplace=True)
cases_evolution_mg.head()

cases_evolution_mg.groupby('FAIXA_ETARIA').count()['EVOLUCAO'].reset_index().sort_values('FAIXA_ETARIA')

cases_evolution_mg.FAIXA_ETARIA = cases_evolution_mg.FAIXA_ETARIA.str.replace('<1ANO','0 A 11 MESES')
cases_evolution_mg.groupby('FAIXA_ETARIA').count()['EVOLUCAO'].reset_index().sort_values('FAIXA_ETARIA')

cases_evolution_mg.info()

"""Após nova verificação das informações do dataframe, ainda existem diversos valores ausentes.

A princípio serão removidos os registros que não contém código do município ("*CODIGO*") e faixa etária ("*FAIXA_ETARIA*") definidos, pois provavelmente serão atributos chave para os modelos de ML futuros.
"""

cases_evolution_mg.dropna(subset=['CODIGO','FAIXA_ETARIA'], inplace=True)
cases_evolution_mg.info()

"""Ainda restaram dados de Unidade Regional de Saúde (URS) além de micro e macro regiões. Vajamos a que se referem."""

missing_cases = cases_evolution_mg[cases_evolution_mg.isna().any(axis=1)]
#missing_cases.describe(datetime_is_numeric=True, include='all')
missing_cases.tail(10)

"""Analisando o dataframe, nota-se que são referentes a casos registrados em outros estados ou até mesmo países.

Serão descartados pois o objetivo deste trabalho é a análise dos casos no estado de Minas Gerais.
"""

cases_evolution_mg.dropna(inplace=True)
cases_evolution_mg.info()

"""Removendo caracteres especiais dos nomes dos municípios."""

# https://stackoverflow.com/a/46197147
cases_evolution_mg['MUNICIPIO_RESIDENCIA'] = cases_evolution_mg['MUNICIPIO_RESIDENCIA'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8').str.upper()
cases_evolution_mg['MUNICIPIO_RESIDENCIA'] = cases_evolution_mg['MUNICIPIO_RESIDENCIA'].replace('-', ' ', regex=True)
cases_evolution_mg['MUNICIPIO_RESIDENCIA'] = cases_evolution_mg['MUNICIPIO_RESIDENCIA'].replace('\'', ' ', regex=True)

"""Reiniciando o índice e alterando os tipos de dados das colunas para mais adequados."""

cases_evolution_mg.reset_index(drop=True, inplace=True)
cases_evolution_mg = cases_evolution_mg.convert_dtypes()
# Não foi identificado corretamente o tipo da coluna DATA_NOTIFICACAO, definindo manualmente
cases_evolution_mg.DATA_NOTIFICACAO = pd.to_datetime(cases_evolution_mg.DATA_NOTIFICACAO)
cases_evolution_mg.info()

"""O primeiro caso confirmado de Covid-19 em Minas Gerais foi em 06/03/2020 conforme publicado pela [Secretatia de Estado da Saúde](https://https://www.saude.mg.gov.br/component/gmg/story/12233-confirmacao-do-primeiro-caso-de-coronavirus-covid-19-em-minas-gerais)."""

first_case_date = pd.to_datetime('2020-03-06')
cases_evolution_mg.query('DATA_NOTIFICACAO < "' + first_case_date.strftime('%Y-%m-%d') + '"')

"""Muitos casos com datas anteriores à 06/03/2020, provavelmente de dados informados incorretamente ou armazenamento em diversos padrões pelos sistemas podendo ter invertido dia com mês, o que pode ter gerado a falha durante a integração de dados.

Datas anteriores a 06/03/2021 não serão descartadas de imediato, porém terão o dia e mês invertidos, gerando uma nova data, desde que o dia da data atual no datarame não seja maior que 12, pois ao alterar para mês seria gerado erro.

Se mesmo assim restarem datas anteriores, serão removidas.
"""

#def correct_dates_before(date, reference):
#  try:
#    if date < reference:
#      date = pd.to_datetime(dt.datetime.strftime(date, '%Y-%d-%m'))
#  except:
#    date
df = cases_evolution_mg.copy()
cases_evolution_mg.DATA_NOTIFICACAO = cases_evolution_mg.DATA_NOTIFICACAO.apply(lambda d: pd.to_datetime(dt.datetime.strftime(d, '%Y-%d-%m')) if (d < first_case_date) & (d.day <= 12) else d)
cases_evolution_mg.drop(cases_evolution_mg.query('DATA_NOTIFICACAO < "' + first_case_date.strftime('%Y-%m-%d') + '"').index, inplace=True)

"""Verificando novamente registros anteriores à 06/03/2020."""

cases_evolution_mg.query('DATA_NOTIFICACAO < "' + first_case_date.strftime('%Y-%m-%d') + '"')

"""Abaixo foi definida uma função de cálculo de casos por local em função da data.

Consegue-se avaliar dia a dia a quantidade de casos por local.

**Retorna um *dataframe*** com as novas colunas calculadas.
"""

def total_cases_by_place(dataframe, place_cols: list(), date_col: str):
  df = dataframe.copy()

  for place_col in place_cols:
    print('Counting by ' + place_col)
    df.sort_values([place_col, date_col], inplace=True)
    df.insert(len(df.columns), "CASOS_DIA_" + place_col, df.groupby([place_col,date_col])[date_col].transform('count'))

  df.sort_values(date_col, inplace=True)
  df.reset_index(drop=True, inplace=True)
  print('Finished')
  return df

cases_evolution_mg = total_cases_by_place(cases_evolution_mg, ['URS','MACRO','MICRO','MUNICIPIO_RESIDENCIA'], 'DATA_NOTIFICACAO')

cases_evolution_mg.query('MUNICIPIO_RESIDENCIA == "ENTRE RIOS DE MINAS"')[['DATA_NOTIFICACAO','MUNICIPIO_RESIDENCIA','CASOS_DIA_MUNICIPIO_RESIDENCIA','CASOS_DIA_MICRO','CASOS_DIA_MACRO','CASOS_DIA_URS']].head()

"""### Dados do IBGE
Verificando agora o dataset com a população de Minas Gerais.
"""

print('Formato:',ibge_cities_mg.shape,'\n')
print('Informações:')
print(ibge_cities_mg.info())

ibge_cities_mg.describe()

"""Observa-se que não há valores ausentes no dataset.

A próxima etapa é eliminar coluna "Gentílico" por ser irrelevante, renomear demais colunas e tratar dados como por exemplo remover texto "pessoas" da coluna de população estimada e alterar tipos de dados das colunas.

Os municípios contém acentos que serão removidos, além de os nomes colocados em caixa alta para manter o padrão do dataset da covid-19.
"""

if 'Gentílico' in ibge_cities_mg.columns: ibge_cities_mg.drop(labels='Gentílico', axis=1, inplace=True)
ibge_cities_mg.columns = ['MUNICIPIO','POP_ESTIMADA_2020','SALARIO_MEDIO_2018','IDHM_2010']
ibge_cities_mg['MUNICIPIO'] = ibge_cities_mg['MUNICIPIO'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8').str.upper()
ibge_cities_mg['MUNICIPIO'] = ibge_cities_mg['MUNICIPIO'].replace('SAO TOME', 'SAO THOME', regex=True)
ibge_cities_mg['MUNICIPIO'] = ibge_cities_mg['MUNICIPIO'].replace('-', ' ', regex=True)
ibge_cities_mg['MUNICIPIO'] = ibge_cities_mg['MUNICIPIO'].replace('\'', ' ', regex=True).astype(str)
ibge_cities_mg['POP_ESTIMADA_2020'] = ibge_cities_mg['POP_ESTIMADA_2020'].replace('\D', '', regex=True).astype(int)
ibge_cities_mg['SALARIO_MEDIO_2018'] = ibge_cities_mg['SALARIO_MEDIO_2018'].replace('[^/0-9,.]', '', regex=True).replace(',', '.', regex=True).astype(float)
ibge_cities_mg['IDHM_2010'] = ibge_cities_mg['IDHM_2010'].replace(',', '.', regex=True).astype(float)
ibge_cities_mg.info()

ibge_cities_mg.sample(5)

"""### Enriquecendo o dataframe da covid-19
Será enriquecido o dataframe da covid-19 com a união com o dataframe de dados do IBGE.

Os dataframe do IBGE não possui o código do múnicípio, portanto a união será feita pelo nome do município.
"""

# Adicionando dados do IBGE
cases_evolution_mg.rename({'MUNICIPIO_RESIDENCIA':'MUNICIPIO','CODIGO':'CODIGO_MUNICIPIO','CASOS_DIA_MUNICIPIO_RESIDENCIA':'CASOS_DIA_MUNICIPIO'}, axis=1, inplace=True)
cases_evolution_mg = cases_evolution_mg.merge(ibge_cities_mg, on=['MUNICIPIO'], how='left')
cases_evolution_mg.head()

cases_evolution_mg.info()

missing_cases = cases_evolution_mg[cases_evolution_mg.isna().any(axis=1)]
missing_cases

ibge_cities_mg.loc[(ibge_cities_mg['MUNICIPIO'].str.contains('DONA EU')) | (ibge_cities_mg['MUNICIPIO'].str.contains('^BRA.*OPOLIS$'))]

"""Nota-se que surgiram valores ausentes nas colunas que vieram do dataframe do IBGE, isso porque o nome no dataframe dos municípios de *Brazópolis* e *Dona Euzébia* estão escritos incorretamente quando comparados aos dados do IBGE.

As colunas adicionadas serão removidas, os nomes corrigidos e a união executada novamente.
"""

cases_evolution_mg.drop(columns=['POP_ESTIMADA_2020','SALARIO_MEDIO_2018','IDHM_2010'], axis=1, inplace=True)
cases_evolution_mg.loc[cases_evolution_mg['MUNICIPIO'].str.contains('DONA EU'), 'MUNICIPIO'] = 'DONA EUZEBIA'
cases_evolution_mg.loc[cases_evolution_mg['MUNICIPIO'].str.contains('^BRA.*OPOLIS$'), 'MUNICIPIO'] = 'BRAZOPOLIS'
cases_evolution_mg = cases_evolution_mg.merge(ibge_cities_mg, on=['MUNICIPIO'], how='left')

cases_evolution_mg.info()

cases_evolution_mg.head()

"""## Análise dos dados"""

# Facilita a plotagem de gráficos mais comuns
def sns_plot(x, y, data, palette='crest', title='', xlabel='', ylabel='', xtickrot=45, kind='line', hue=None, figsize=(12,8)):
  plt.rc("figure", figsize=figsize) # tamanho padrão de plotagem

  if kind == 'line':
    g = sns.lineplot(x=x, y=y, palette=palette, hue=hue, data=data)
  elif kind == 'bar':
    g = sns.barplot(x=x, y=y, palette=palette, hue=hue, data=data)
  elif kind == 'hist':
    g = sns.histplot(x=x, y=y, palette=palette, hue=hue, data=data)
  elif kind == 'box':
    g = sns.boxplot(x=x, y=y, palette=palette, hue=hue, data=data)
  else:
    raise ValueError('Invalid kind. Suported: line, bar, hist, box')
  
  g.set_xlabel(xlabel)
  g.set_ylabel(ylabel)
  g.set_title(title, {'fontsize': 14,'fontweight' : 'bold'})

  if xtickrot > 0:
    g.set_xticklabels(g.get_xticklabels(), rotation=xtickrot, horizontalalignment='right')
  
  plt.plot()

"""### Hipótese

Definindo a hipótese nula (H<sub>0</sub>) e a alternativa (H<sub>1</sub>)

H<sub>0</sub>: evoluir para óbito <= 6,4%

H<sub>1</sub>: evoluir para óbito > 6,4%

α = 5% (nível de significância)

p-valor = ? (valores muito pequenos evidenciam que a  H<sub>0</sub> provavelmente é falsa, pode-se considerar p-valor pequeno quando o mesmo é menor ou igual a α)

Rejeitar H<sub>0</sub> implica em aceitar H<sub>1</sub>
"""

from statsmodels.stats.proportion import proportions_ztest
deaths = cases_evolution_mg.query("EVOLUCAO == 'OBITO'")['EVOLUCAO'].count()
total = cases_evolution_mg['EVOLUCAO'].count()
stat, p = proportions_ztest(deaths, total, value=0.064, alternative='larger') # alternatives: [‘two-sided’, ‘smaller’, ‘larger’]
print('Estatística de teste: {}'.format(stat.round(2)))
print('p-valor: {}'.format(p.round(2)))

# Proporção de mortes x recuperados
deaths/total

"""P-valor é maior que o nível de significância definido, portanto H<sub>0</sub> será aceita.

Avaliação de óbitos comparados à internações.
"""

death_count = cases_evolution_mg.query("EVOLUCAO == 'OBITO'").groupby(['INTERNACAO','SEXO']).count()['EVOLUCAO'].reset_index()
death_count

sns_plot(data=death_count, x='INTERNACAO', y='EVOLUCAO', hue='SEXO', kind='bar', palette='Reds_r', title='Óbitos x Internações', xlabel='Óbito', ylabel='Casos', xtickrot=0)

"""Curiosamente, todas as pessoas que evoluíram para óbito no dataset foram internadas, isso é um fator negativo para uso em modelo de ML posterior pois é tendencioso, ou seja, pode induzir o modelo a classificar internados como pessoas que evoluirão para óbito, removendo a capacidade de generalização.

Se todos que evoluíram para óbito foram internados, será que algo semelhante acontece para os que se recuperaram?

Serão verificados os recuperados comparados às internações.
"""

recovered_count = cases_evolution_mg.query("EVOLUCAO == 'RECUPERADO'").groupby(['INTERNACAO','SEXO']).count()['EVOLUCAO'].reset_index()
recovered_count

sns_plot(data=recovered_count, x='INTERNACAO', y='EVOLUCAO', hue='SEXO', kind='bar', palette='Reds_r', title='Recuperados x Internações', xlabel='Internação', ylabel='Casos', xtickrot=0)

"""Neste caso a situação se inverte, a maior parte dos recuperados nao precisou de internação.

Em ambas análises a quantidade de pessoas cujo sexo não foi informado é extremamente pequena, não gerando na plotagem a coluna.
"""

age_range_evolution = cases_evolution_mg.groupby(['FAIXA_ETARIA','EVOLUCAO']).count()['INTERNACAO'].reset_index().sort_values(['FAIXA_ETARIA','EVOLUCAO'], ascending=[True, False]).rename({'INTERNACAO':'CASOS'}, axis=1)
age_range_evolution.head()

sns_plot(data=age_range_evolution, x='FAIXA_ETARIA', y='CASOS', hue='EVOLUCAO', kind='bar', palette='Reds', title='Evolução x Faixa Etária', xlabel='Faixa Etária', ylabel='Casos')

cases_by_race = cases_evolution_mg.groupby(['RACA']).count()['EVOLUCAO'].reset_index().sort_values(['EVOLUCAO'], ascending=False).rename({'EVOLUCAO':'CASOS'}, axis=1)
cases_by_race.head()

sns_plot(data=cases_by_race, x='RACA', y='CASOS', kind='bar', palette='Reds_r', title='Casos x Raça', xlabel='Raça', ylabel='Casos')

cases_by_macro = cases_evolution_mg.groupby(['MACRO']).count()['EVOLUCAO'].reset_index().sort_values(['EVOLUCAO'], ascending=False).rename({'EVOLUCAO':'CASOS'}, axis=1)
cases_by_macro.head()

sns_plot(data=cases_by_macro, x='MACRO', y='CASOS', kind='bar', palette='Reds_r', title='Casos x Macro Região', xlabel='Macro Região', ylabel='Casos')

"""### Correlação

Mapa da correlação de Pearson.
"""

# https://seaborn.pydata.org/examples/heat_scatter.html?highlight=corr
corr_matrix = cases_evolution_mg.corr().stack().reset_index(name="correlation")
g = sns.relplot(
    data=corr_matrix,
    x="level_0", y="level_1", hue="correlation", size="correlation",
    palette="vlag", hue_norm=(-1, 1), edgecolor=".7",
    height=8, sizes=(50, 500), size_norm=(-.2, .8),
)
g.set(xlabel="", ylabel="", aspect="equal")
g.despine(left=True, bottom=True)
#g.ax.margins(.02)
for label in g.ax.get_xticklabels():
    label.set(rotation=45,horizontalalignment='right')

"""## Machine Learning

Nesta seção o objetivo é definir o melhor modelo de ML para prever recuperações e óbitos.

Importação de bibliotecas.
"""

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.feature_extraction import DictVectorizer

# Bibliotecas de modelos
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
#from sklearn.linear_model import LogisticRegression
#from sklearn.naive_bayes import GaussianNB

"""### Pré processsamento para ML

Algumas colunas são irrelevantes para os modelos de ML, um exemplo é o aumento da dimensionalidade, ou seja, número de colunas. Portanto algumas remoções serão realizadas.
"""

# Criando um DF para armazenar os códigos de municípios, municípios, URSs, macro e micro regiões para eventual uso futuro
cols_to_drop = ['CODIGO_MUNICIPIO','MUNICIPIO','URS','MACRO','MICRO']
mg_cities = cases_evolution_mg[cols_to_drop].sort_values('CODIGO_MUNICIPIO').drop_duplicates().reset_index(drop=True)
mg_cities.head()

# Novo dataframe para ML sem algumas colunas
ml_cases_evolution_mg = cases_evolution_mg.drop(columns=cols_to_drop)
ml_cases_evolution_mg.drop(columns=['DATA_NOTIFICACAO'], inplace=True)
ml_cases_evolution_mg.info()

"""Alguns modelos não trabalham com dados categóricos e precisam ser convertidos.

Label encoder: transforma classes de dados categóricos em números que os representam.

One-hot encoder: cada classe de dado categórico é convertida em uma nova coluna e aplica o valor 0 (zero) ou 1 (um).

O One-hot encoder, quando aplicado corretamente pode melhorar os modelos, porém gera aumento de dimensionalidade (mais colunas).

Dessa forma deve-se avaliar a cardinalidade, ou seja, a quantidade de registros únicos de uma coluna, por exemplo, uma coluna Sexo, que contém apenas M ou F, possui cardinalidade 2. Portanto apenas variáveis categóricas de baixa cardinalidade são candidatas a serem convertidas com o One-hot encoder, as demais usarão o Label encoder.
"""

# Label encoder x One-hot encoder
categ_cols = ml_cases_evolution_mg.select_dtypes(include=['string']).columns

# Colunas para one-hot encoder
low_cardinality_cols = [col for col in categ_cols if ml_cases_evolution_mg[col].nunique() <= 3]
low_cardinality_cols.remove('EVOLUCAO') # Variável resposta será codificada com o label encoder

# Colunas para label encoder
high_cardinality_cols = list(set(categ_cols)-set(low_cardinality_cols))

# Apply one-hot encoder to each column with categorical data
oh_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
oh_df = pd.DataFrame(oh_encoder.fit_transform(ml_cases_evolution_mg[low_cardinality_cols]), columns=oh_encoder.get_feature_names(low_cardinality_cols))
oh_df.index = ml_cases_evolution_mg.index
oh_df.head()

ml_cases_evolution_mg.drop(columns=low_cardinality_cols, inplace=True)
label_encoders = dict()
for col in high_cardinality_cols:
  le = LabelEncoder()
  ml_cases_evolution_mg[col] = le.fit_transform(ml_cases_evolution_mg[col])
  label_encoders[col] = le
ml_cases_evolution_mg = pd.concat([ml_cases_evolution_mg,oh_df],axis=1)
ml_cases_evolution_mg.head()

label_encoders

# Teste do label encoder
label_encoders['FAIXA_ETARIA'].inverse_transform(ml_cases_evolution_mg.FAIXA_ETARIA)

# Apenas visualizando o gráfico atual de correlação
# https://seaborn.pydata.org/examples/heat_scatter.html?highlight=corr
corr_matrix = ml_cases_evolution_mg.corr().stack().reset_index(name="correlation")
g = sns.relplot(
    data=corr_matrix,
    x="level_0", y="level_1", hue="correlation", size="correlation",
    palette="vlag", hue_norm=(-1, 1), edgecolor=".7",
    height=10, sizes=(50, 300), size_norm=(-.2, .8),
)
g.set(xlabel="", ylabel="", aspect="equal")
g.despine(left=True, bottom=True)
#g.ax.margins(.02)
for label in g.ax.get_xticklabels():
    label.set(rotation=45,horizontalalignment='right')

"""### Balanceamento

Um dos grandes problemas de classificadores é o de amostras desbalanceadas, ou seja, uma discrepância muito grande nos itens da variável resposta.

Há uma outra situação que, ao deixar em igualdade a quantidade de itens da variável resposta podemos perder capacidade de previsão, portanto é necessário ter esse *tradeoff* em mente.

Como visto na seção de análise dos dados, a quantidade de **recuperados** é muito superior à de **óbitos**. Serão removidos eventuais linhas duplicadas do *dataset* e, posteriormente haverá o balanceamento.
"""

print('Formatos do dataframe\n')
print('Original: ' + str(ml_cases_evolution_mg.shape))
ml_cases_evolution_mg.drop_duplicates(inplace=True)
ml_cases_evolution_mg.reset_index(drop=True, inplace=True)
print('Apóes duplicadas removidas: ' + str(ml_cases_evolution_mg.shape))

# Contagem de óbitos
# Óbito = 0 | Recuperado = 1
ml_total_deaths = ml_cases_evolution_mg.EVOLUCAO[ml_cases_evolution_mg.EVOLUCAO == 0].count()
ml_total_deaths

# Mantendo dataset não balanceado para eventual comparação
nb_ml_cases_evolution_mg = ml_cases_evolution_mg.copy()
# Balanceamento aleatório para óbitos e recuperados possuírem as mesmas quantidades
ml_cases_evolution_mg = pd.concat([ml_cases_evolution_mg.query('EVOLUCAO == 1').sample(ml_total_deaths),ml_cases_evolution_mg.query('EVOLUCAO == 0')], axis=0)
ml_cases_evolution_mg.reset_index(drop=True,inplace=True)
print('Balanceamento aleatório - quantidade de itens')
print(ml_cases_evolution_mg.EVOLUCAO.value_counts())

ml_cases_evolution_mg.head()

"""### Separação do treino e teste"""

# Criando váriáveis X e y
X = ml_cases_evolution_mg.drop('EVOLUCAO', 1)
y = ml_cases_evolution_mg['EVOLUCAO']

# Dataframe não balanceado
nb_X = nb_ml_cases_evolution_mg.drop('EVOLUCAO', 1)
nb_y = nb_ml_cases_evolution_mg['EVOLUCAO']

# Separando em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)

# Treino e teste não balanceado
nb_X_train, nb_X_test, nb_y_train, nb_y_test = train_test_split(nb_X, nb_y, test_size = 0.3)

print("Treino - X: " + str(X_train.shape) + " | y: " + str(y_train.shape))
print("Teste - X: " + str(X_test.shape) + " | y: " + str(y_test.shape))

"""### Modelos"""

# Automatiza a execução dos modelos e obtenção dos resultados
def run_classifier(classifier, Xtrain, Xtest, ytrain, ytest, target_labels = None):
  results = list()

  params_tree = {
      'criterion': ['gini','entropy'],
      'max_depth': [5, 15]
  }
  params_svc = {
      'kernel': ['poly', 'rbf', 'sigmoid']
  }
  params_xgb = {
      'n_estimators': [100, 150]
  }
  params_gbc = {
      'n_estimators': [100, 150]
  }
  params_knc = {
      'n_neighbors': [3, 5]
  }

  if type(classifier) != list:
    classifier = [classifier]

  for clf in classifier:
    model_name = clf.__class__.__name__

    if (model_name == 'DecisionTreeClassifier') | (model_name == 'RandomForestClassifier'):
      params = params_tree
    elif model_name == 'GaussianNB':
      params = {}
    elif model_name == 'SVC':
      params = params_svc
    elif model_name == 'XGBClassifier':
      params = params_xgb
    elif model_name == 'GradientBoostingClassifier':
      params = params_gbc
    elif model_name == 'KNeighborsClassifier':
      params = params_knc
    else:
      raise Exception('Model not yet supported. Supported models: GaussianNB, DecisionTreeClassifier, RandomForestClassifier, SVC, XGBClassifier, GradientBoostingClassifier, KNeighborsClassifier.')
    
    grid_search = GridSearchCV(estimator=clf,param_grid=params)
    grid_search.fit(Xtrain, ytrain)
    predictions = grid_search.predict(Xtest)
    accuracy = accuracy_score(ytest,predictions)
    print(f'----------- MODELO: {model_name} -----------\n')
    print(f'Melhores parâmetros: {grid_search.best_params_}\n')
    print('Matriz de confusão:\n',pd.DataFrame(data=confusion_matrix(ytest, predictions), index=target_labels, columns=target_labels))
    print(f'\nAcurácia:\n{(accuracy*100):.2f}% ({accuracy})')
    print('\nRelatório de classificação:\n',classification_report(ytest, predictions, target_names=target_labels))
    print(f'------------------------------------------------------\n\n\n')
    result = dict()
    result['model'] = model_name
    result['best_params'] = grid_search.best_params_
    result['params'] = grid_search.best_estimator_.get_params()
    result['accuracy'] = accuracy
    result['report'] = classification_report(ytest, predictions, target_names=target_labels, output_dict=True)
    results.append(result)

  return results

# Modelos a serem usados
model_gsn = GaussianNB()
model_dtc = DecisionTreeClassifier()
model_rfc = RandomForestClassifier()
model_svm = SVC()
model_xgb = XGBClassifier()
model_gbc = GradientBoostingClassifier()
model_knc = KNeighborsClassifier()

"""#### Execução

Comparação entre a amostra balanceada e não balanceda usando árvore de decisão com configurações padrão.
"""

def decision_tree(X,y,Xt,yt):
  dt_model = DecisionTreeClassifier()
  dt_model.fit(X, y)
  dt_predictions = dt_model.predict(Xt)
  accuracy = accuracy_score(yt,dt_predictions)
  print('Matriz de confusão:\n',pd.DataFrame(data=confusion_matrix(yt, dt_predictions), index=['OBITO','RECUPERADO'], columns=['OBITO','RECUPERADO']))
  print(f'\nAcurácia:\n{(accuracy*100):.2f}% ({accuracy})')
  print('\nRelatório de classificação:\n',classification_report(yt, dt_predictions, target_names=['OBITO','RECUPERADO']))
  #print(plot_tree(dt_model))

print("##### Não Balanceado #####")
decision_tree(nb_X_train,nb_y_train,nb_X_test,nb_y_test)
print("\n\n##### Balanceado #####")
decision_tree(X_train,y_train,X_test,y_test)

"""Se for avaliado apenas a acurácia, o modelo com a amostra não balanceada poderia ser considerado o melhor, porém acurária não é tudo.

Nota-se, especialmente pela precisão e f1-score, que o desempenho da amostra balanceada é melhor que da não balanceada, ou seja, houveram mais acertos em ambas classes.

Agora ocorrerá a xecução de diversos classificadores com a amostra balanceada.
"""

models_results = run_classifier([model_gsn,model_dtc,model_rfc,model_xgb,model_gbc,model_knc],X_train,X_test,y_train,y_test,target_labels=['OBITO','RECUPERADO'])

"""#### Verificação dos resultados"""

models_results

print('##### Acurácias #####')
for model_result in models_results:
  print(f"{model_result['model']}: {(model_result['accuracy']*100):.2f}%")

"""## Apresentação dos resultados

Convertendo resultados para *Dataframe*.
"""

rows = list()
for model_result in models_results:
  item = list()
  item.append(model_result['model'])
  item.append(model_result['accuracy'])
  item.append(model_result['report']['OBITO']['f1-score'])
  item.append(model_result['report']['OBITO']['precision'])
  item.append(model_result['report']['OBITO']['recall'])
  item.append(model_result['report']['OBITO']['support'])
  item.append(model_result['report']['RECUPERADO']['f1-score'])
  item.append(model_result['report']['RECUPERADO']['precision'])
  item.append(model_result['report']['RECUPERADO']['recall'])
  item.append(model_result['report']['RECUPERADO']['support'])
  rows.append(item)

df_models_results = pd.DataFrame(rows, columns=['modelo','acurácia','f1-score óbito','precisão óbito','revocação óbito','suporte óbito',
                                                'f1-score recuperado','precisão recuperado','revocação recuperado','suporte recuperado'])
df_models_results

"""### Gráficos"""

sns_plot(x='modelo', y='acurácia', palette='crest', data=df_models_results.sort_values('acurácia', ascending=False), title='Acurácia dos modelos', xlabel='Modelo', ylabel='Acurácia', kind='bar')

sns_plot(x='modelo', y='f1-score óbito', palette='crest', data=df_models_results.sort_values('f1-score óbito', ascending=False), title='f1-score óbito dos modelos', xlabel='Modelo', ylabel='f1-score', kind='bar')

sns_plot(x='modelo', y='f1-score recuperado', palette='crest', data=df_models_results.sort_values('f1-score recuperado', ascending=False), title='f1-score recuperado dos modelos', xlabel='Modelo', ylabel='f1-score', kind='bar')

sns_plot(x='modelo', y='precisão óbito', palette='crest', data=df_models_results.sort_values('precisão óbito', ascending=False), title='Precisão óbito dos modelos', xlabel='Modelo', ylabel='Precisão', kind='bar')

sns_plot(x='modelo', y='precisão recuperado', palette='crest', data=df_models_results.sort_values('precisão recuperado', ascending=False), title='Precisão recuperado dos modelos', xlabel='Modelo', ylabel='Precisão', kind='bar')

"""Temos um vencedor!

O modelo que apresentou melhor desempenho, baseado na avaliação dos resultados, em especial f1-score foi o **GradientBostClassifier**, seguido de perto pelo **XGBClassifier** e em terceiro lugar o **RandomForestClassifier**.

Ranking:

1.   GradientBostClassifier
2.   XGBClassifier
3.   RandomForestClassifier


"""

df_models_results.sort_values(['f1-score óbito','f1-score recuperado'], ascending=[False,False]).reset_index(drop=True)